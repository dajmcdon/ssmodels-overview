---
title: "Quick intro to State Space models"
author: "Daniel J. McDonald"
date: "02 June 2022"
output:
  xaringan::moon_reader:
    css: [src/xaringan-themer.css, src/slides-style.css]
    nature:
      beforeInit: ["src/macros.js"]
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
    seal: false
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(fontawesome)
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  dev = "svg",
  fig.path = "gfx/",
  fig.align = 'center',
  fig.width=9, fig.height=3.5, fig.retina=3,
  fig.showtext = TRUE,
  out.width = "100%",
  cache = TRUE,
  autodep = TRUE,
  echo = FALSE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
primary = "#598234"
secondary = "#68829e"
tertiary = "#ffa319"
fourth_color = "#DB0B5B"

style_duo_accent(
  primary_color      = primary,  #"#002145", # UBC primary
  secondary_color    = secondary,  #"6EC4E8", # UBC secondary 4
  header_font_google = google_font("EB Garamond"),
  text_font_google = google_font("Open Sans"),
  code_font_google = google_font("Fira Mono"),
  text_color = "#000000",
  table_row_even_background_color = lighten_color(primary, 0.8),
  colors = c(
    tertiary = tertiary, fourth_color = fourth_color,
    light_pri = lighten_color(primary, 0.8),
    light_sec = lighten_color(secondary, 0.8),
    light_ter = lighten_color(tertiary, 0.8),
    light_fou = lighten_color(fourth_color, 0.8)
    ),
  outfile = here::here("src/xaringan-themer.css")
)
# theme_set(theme_xaringan())
```

layout: true

<div class="my-footer"><span><a href="https://dajmcdon.github.io/talktemplate" style="color:white">dajmcdon.github.io/ssmodels-overview</a></span></div> 

---
background-image: url("gfx/cover.svg")
background-size: contain
background-position: top

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>



.center[# Quick overview of state space models]


.pull-left[
###Daniel J. McDonald
###University of British Columbia
####Reading Group
]

---

## Generic state space model

.pull-left[

<br><br><br>

$$\begin{aligned} x_k &\sim p(x_k | x_{k-1}) \\ y_k &\sim p(y_k | x_k)\end{aligned}$$

]

.pull-right[

* $x_k$ is unobserved, dimension $n$

* $y_k$ is observed, dimension $m$

* $x$ process is the __transition__ or __process__ equation

* $y$ is the __observation__ or __measurement__ equation

* Both are probability distributions that can depend on parameters $\theta$

* For now, assume $\theta$ is **KNOWN**

* We can allow the densities to vary with time.

]

--

**GOAL(s)**

1. Filtering: given observations, find $p(x_k | y_1,\ldots y_k)$

1. Smoothing: given observations, find $p(x_k | y_1,\ldots y_T)$, $k < T$

1. Forecasting: given observations, find $p(y_{k+1} | y_1,\ldots,y_k)$

---

## Using Bayes Rule

Assume $p(x_0)$ is known

$$p(y_1,\ldots,y_T\ |\ x_1, \ldots, x_T) = \prod_{k=1}^T p(y_k | x_k)$$

$$p(x_0,\ldots,x_T) = p(x_0) \prod_{k=1}^T p(x_k | x_{k-1})$$

$$\begin{aligned}p(x_0,\ldots,x_T\ |\ y_1,\ldots,y_T) &= \frac{p(y_1,\ldots,y_T\ |\ x_1, \ldots, x_T)p(x_0,\ldots,x_T)}{p(y_1,\ldots,y_T)}\\ &\propto p(y_1,\ldots,y_T\ |\ x_1, \ldots, x_T)p(x_0,\ldots,x_T)\end{aligned}$$

In principle, if things are nice, you can compute this posterior (thinking of $x$ as unknown parameters)

But in practice, computing a big multivariate posterior like this is computationally ill-advised.

---

## Generic filtering

* Recursively build up $p(x_k | y_1,\ldots y_k)$.

* Why? Because if we're collecting data in real time, this is all we need to make forecasts for future data.

$$\begin{aligned} &p(y_{T+1} | y_1,\ldots,y_T)\\ &= p(y_{T+1} | x_{T+1}, y_1,\ldots,y_T)\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | y_1,\ldots,y_T)\\ &= p(y_{T+1} | x_{T+1} )p(x_{T+1} | x_T) p(x_T | y_1,\ldots,y_T)\end{aligned}$$

* Can continue to iterate if I want to predict $h$ steps ahead

$$\begin{aligned} &p(y_{T+h} | y_1,\ldots,y_T)= p(y_{T+h} | x_{T+h} )\prod_{j=0}^{h-1} p(x_{T+j+1} | x_{T+j}) p(x_T | y_1,\ldots,y_T)\end{aligned}$$

---

## The filtering recursion

1. Initialization. Fix $p(x_0)$.

Iterate the following for $k=1,\ldots,T$:

2. Predict. $$p(x_k | y_{k-1}) = \int p(x_k | x_{k-1}) p(x_{k-1} | y_1,\ldots, y_{k-1})dx_{k-1}.$$

3. Update. $$p(x_k | y_1,\ldots,y_k) = \frac{p(y_k | x_k)p(x_k | y_1,\ldots,y_{k-1})}{p(y_1,\ldots,y_k)}$$

--

In general, this is somewhat annoying because these integrals may be challenging to solve.

But with some creativity, we can use Monte Carlo for everything.

---

## What if we make lots of assumptions?

Assume that $$\begin{aligned}p(x_0) &= N(m_0, P_0) \\ p_k(x_k | x_{k-1}) &= N(A_{k-1}x_{k-1}, Q_{k-1})\\ p_k(y_k | x_k) &= N(H_k x_k, R_k)\end{aligned}.$$

Then __all the ugly integrals have closed form representations__ by properties of conditional Gaussian distributions.

--

.pull-left[
Distributions:

$$
\begin{aligned}
p(x_k | y_1,\ldots,y_{k-1}) &= N(m^{-}_k, P^{-}_k)\\
p(x_k | y_1,\ldots,y_{k}) &= N(m_k, P_k)\\
p(y_{k} | y_1,\ldots,y_{k-1}) &= N(H_k m^-_k, S_k)\\
\end{aligned}
$$
Prediction:
$$
\begin{aligned}
m^-_k &= A_{k-1}m_{k-1}\\
P^-_k &= A_{k-1}P_{k-1}A^\mathsf{T}_{k-1} + Q_{k-1}
\end{aligned}
$$

]

.pull-right[
Update:
$$
\begin{aligned}
v_k &= y_k - H_k m_k^-\\
S_k &= H_k P_k^- H_k^\mathsf{T} + R_k\\
K_k &= P^-_k H_k^\mathsf{T} S_k^{-1}\\
m_k &= m^-_k + K_{k}v_{k}\\
P_k &= P^-_k - K_k S_k K_k^\mathsf{T}
\end{aligned}
$$

]

---

## Code or it isn't real (Kalman Filter)

.pull-left[
```{r kalman-filter, echo=TRUE}
kalman <- function(y, m0, P0, A, Q, H, R) {
  n <- length(y)
  m <- double(n+1)
  P <- double(n+1)
  m[1] <- m0
  P[1] <- P0
  for (k in seq(n)) {
    mm <- A * m[k]
    Pm <- A * P[k] * A + Q
    v <- y[k] - H * mm
    S <- H * Pm * H + R
    K <- Pm * H / S
    m[k+1] <- mm + K * v
    P[k+1] <- Pm - K * S * K
  }
  tibble(t = 1:n, m = m[-1], P = P[-1])
}
```
]

.pull-right[
```{r simul, echo=TRUE}
set.seed(2022-06-01)
x <- double(100)
for (k in 2:100) x[k] = x[k-1] + rnorm(1)
y <- x + rnorm(100, sd = 1)
kf <- kalman(y, 0, 5, 1, 1, 1, 1)
```

```{r plot-it, fig.height=3, fig.width=4}
ggplot(bind_cols(kf, x=x, y=y), aes(t)) +
  geom_ribbon(aes(ymax=m+2*P, ymin=m-2*P), alpha = .2) +
  geom_line(aes(y=m, color = "kf_mean")) +
  geom_line(aes(y=x, color = "x")) +
  geom_point(aes(y=y, color = "y")) +
  theme_bw() +
  scale_color_manual(breaks = c("kf_mean", "x", "y"),
                     values = c(kf_mean = "black", x = primary, y=secondary)) +
  theme(legend.position = "bottom", axis.title=element_blank())
```



]


---

## Important notes

* So far, we assumed all parameters were known

* "Parameters" here is a bit ambiguous: what is $x_k$?

* In reality, we had 6: `m0, P0, A, Q, H, R`

* I sort of also think of $x$ as "parameters" in the Bayesian sense

* By that I mean, "latent variables for which we have prior distributions"

* What if we want to estimate them?

__Bayesian way__: `m0` and `P0` are already the parameters of for the prior on 
$x_1$. Put priors on the other 4.

__Frequentist way__: Just maximize the likelihood. Can technically take 
`P0` $\rightarrow\infty$ to remove it and `m0`

--

The Likelihood is produced as a by-product of the Kalman Filter. 

$$-\ell(\theta) = \sum_{k=1}^T \left(v_k^\mathsf{T}S_k^{-1}v_k + \log |S_k| + m \log 2\pi\right)$$


---

## Smoothing (not in the chapter, whoops...)

* We also want $p(x_k | y_1,\ldots,y_{T})$

* Filtering went "forward" in time. At the end we got, 
$p(x_T | y_1,\ldots,y_{T})$. Smoothing starts there and goes "backward"

* For "everything linear Gaussian", this is again "easy"

* Set $m_T^s = m_T$, $P_T^s = P_T$. 

* For $k = T-1,\ldots,1$, 


$$\begin{aligned}
G_k &= P_k A_k^\mathsf{T} [P_{k+1}^-]^{-1}\\
m_k^s &= m_k + G_k(m_{k+1}^s - m_{k+1}^-)\\
P_k^s &= P_k + G_k(P_{k+1}^s - P_{k+1}^-)G_k^\mathsf{T}\\
x_k | y_1,\ldots,y_T &= N(m^s_k, P_k^s)
\end{aligned}$$

---

## Comparing the filter and the smoother

* Same data, different code (using a package)

```{r kalman-smoothing, echo = TRUE}
library(FKF)
filt <- fkf(a0 = 0, P0 = matrix(5), dt = matrix(0), ct = matrix(0), 
            Tt = matrix(1), Zt = matrix(1), HHt = matrix(1), GGt = matrix(1), 
            yt = matrix(y, ncol = length(y)))
smo <- fks(filt)
```

```{r plot-smooth, fig.height=3}
tibble(time = seq_along(x), x = x, filter = c(filt$att), smooth = c(smo$ahatt)) %>%
  pivot_longer(-time) %>%
  ggplot(aes(time, value, color = name)) + 
  geom_line() +
  theme_bw() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom", axis.title=element_blank(), 
        legend.title = element_blank())
```

---

## What about non-linear and/or non-Gaussian

$$\begin{aligned} x_k &\sim p(x_k | x_{k-1}) \\ y_k &\sim p(y_k | x_k)\end{aligned}$$

Then we need to solve integrals. This is a pain. We approximate them. 

These all give **approximations to the filtering distribution**

* Extended Kalman filter - basically do a Taylor approximation, then do Kalman like
* Uncented Kalman filter - Approximate integrals with Sigma points
* Particle filter - Sequential Monte Carlo
* Bootstrap filter (simple version of SMC)
* Laplace Gaussian filter - Do a Laplace approximation to the distributions

---

## The bootstrap filter

* Need to **simulate** from the transition distribution (`rtrans`)
* Need to **evaluate** the observation distribution (`dobs`)

```{r bootstrap-filter, echo = TRUE}
boot_filter <- 
  function(y, B = 1000, rtrans, dobs, a0 = 0, P0 = 1, perturb = function(x) x) 
    {
    n <- length(y)
    filter_est <- matrix(0, n, B)
    predict_est <- matrix(0, n, B)
    init <- rnorm(B, a0, P0)
    filter_est[1, ] = init
    for (i in seq(n)) {
      raw_w <- dobs(y[i], filter_est[i, ])
      w <- raw_w / sum(raw_w)
      selection <- sample.int(B, replace = TRUE, prob = w)
      filter_est[i, ] <- perturb(filter_est[i, selection])
      predict_est[i, ] <- rtrans(filter_est[i, ])
      if (i < n) filter_est[i+1, ] <- predict_est[i, ]
    }
    list(filt = filter_est, pred = predict_est)
  }
```

---

## About the bootstrap filter

* Super simple. Works by using Monte Carlo approximations for all the integrals
* "Plug and play". All you do is specify 2 functions.
* Likelihood is a direct consequence.
* So if the functions depend on parameters, that's cool.

In the SIR:

* `rtrans =  ode_solve(x[t-1], theta)` - numerically propagates 1-step forward
* `dobs = dnbinom(y[t], x[t], theta)` - evaluate the 1-step likelihood

How do we estimate the parameters $\theta$?

* 0-th order optimization. Want to minimize $f(\theta)$, just evaluate $f$ for $\theta \in \{\theta_1,\ldots,\theta_{huge}\}$, keep the best. `dobs` is the likelihood.
* 1-st order. Need $\nabla_\theta f(\theta)$.
* Bayesian. Like 0-th order, but we explore $\Theta$ in a careful manner.

---

## Final notes

Suppose
$$\begin{aligned}
x_k &= Bx_{k-1} + \epsilon_k\\
y_k &= Ax_{k} + \eta_k
\end{aligned}$$

We can estimate $x_1,\ldots,x_T$ with the Bootstrap filter.

--

$$\hat{x} = \arg\min_x \frac{1}{2}\sum_{k=1}^T (y_k-Ax_k)^2 + \frac{\lambda}{2}
\sum_{k=2}^T(x_k - Bx_{k-1})^2$$

Here, $\lambda = \sigma^2_\eta / \sigma^2_\epsilon$.

This is just "ridge regression". Exactly the same as "just compute $p(x_1,\ldots,x_T | y_1,\ldots,y_T)$".

> if we don't care about the whole distribution, just write it as a penalized likelihood problem

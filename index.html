<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Quick intro to State Space models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Daniel J. McDonald" />
    <meta name="date" content="2022-06-02" />
    <script src="index_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="src/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="src/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">






layout: true

&lt;div class="my-footer"&gt;&lt;span&gt;&lt;a href="https://dajmcdon.github.io/talktemplate" style="color:white"&gt;dajmcdon.github.io/ssmodels-overview&lt;/a&gt;&lt;/span&gt;&lt;/div&gt; 

---
background-image: url("gfx/cover.svg")
background-size: contain
background-position: top

&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;



.center[# Quick overview of state space models]


.pull-left[
###Daniel J. McDonald
###University of British Columbia
####Reading Group
]

---

## Generic state space model

.pull-left[

&lt;br&gt;&lt;br&gt;&lt;br&gt;

`$$\begin{aligned} x_k &amp;\sim p(x_k | x_{k-1}) \\ y_k &amp;\sim p(y_k | x_k)\end{aligned}$$`

]

.pull-right[

* `\(x_k\)` is unobserved, dimension `\(n\)`

* `\(y_k\)` is observed, dimension `\(m\)`

* `\(x\)` process is the __transition__ or __process__ equation

* `\(y\)` is the __observation__ or __measurement__ equation

* Both are probability distributions that can depend on parameters `\(\theta\)`

* For now, assume `\(\theta\)` is **KNOWN**

* We can allow the densities to vary with time.

]

--

**GOAL(s)**

1. Filtering: given observations, find `\(p(x_k | y_1,\ldots y_k)\)`

1. Smoothing: given observations, find `\(p(x_k | y_1,\ldots y_T)\)`, `\(k &lt; T\)`

1. Forecasting: given observations, find `\(p(y_{k+1} | y_1,\ldots,y_k)\)`

---

## Using Bayes Rule

Assume `\(p(x_0)\)` is known

`$$p(y_1,\ldots,y_T\ |\ x_1, \ldots, x_T) = \prod_{k=1}^T p(y_k | x_k)$$`

`$$p(x_0,\ldots,x_T) = p(x_0) \prod_{k=1}^T p(x_k | x_{k-1})$$`

`$$\begin{aligned}p(x_0,\ldots,x_T\ |\ y_1,\ldots,y_T) &amp;= \frac{p(y_1,\ldots,y_T\ |\ x_1, \ldots, x_T)p(x_0,\ldots,x_T)}{p(y_1,\ldots,y_T)}\\ &amp;\propto p(y_1,\ldots,y_T\ |\ x_1, \ldots, x_T)p(x_0,\ldots,x_T)\end{aligned}$$`

In principle, if things are nice, you can compute this posterior (thinking of `\(x\)` as unknown parameters)

But in practice, computing a big multivariate posterior like this is computationally ill-advised.

---

## Generic filtering

* Recursively build up `\(p(x_k | y_1,\ldots y_k)\)`.

* Why? Because if we're collecting data in real time, this is all we need to make forecasts for future data.

`$$\begin{aligned} &amp;p(y_{T+1} | y_1,\ldots,y_T)\\ &amp;= p(y_{T+1} | x_{T+1}, y_1,\ldots,y_T)\\ &amp;= p(y_{T+1} | x_{T+1} )p(x_{T+1} | y_1,\ldots,y_T)\\ &amp;= p(y_{T+1} | x_{T+1} )p(x_{T+1} | x_T) p(x_T | y_1,\ldots,y_T)\end{aligned}$$`

* Can continue to iterate if I want to predict `\(h\)` steps ahead

`$$\begin{aligned} &amp;p(y_{T+h} | y_1,\ldots,y_T)= p(y_{T+h} | x_{T+h} )\prod_{j=0}^{h-1} p(x_{T+j+1} | x_{T+j}) p(x_T | y_1,\ldots,y_T)\end{aligned}$$`

---

## The filtering recursion

1. Initialization. Fix `\(p(x_0)\)`.

Iterate the following for `\(k=1,\ldots,T\)`:

2. Predict. `$$p(x_k | y_{k-1}) = \int p(x_k | x_{k-1}) p(x_{k-1} | y_1,\ldots, y_{k-1})dx_{k-1}.$$`

3. Update. `$$p(x_k | y_1,\ldots,y_k) = \frac{p(y_k | x_k)p(x_k | y_1,\ldots,y_{k-1})}{p(y_1,\ldots,y_k)}$$`

--

In general, this is somewhat annoying because these integrals may be challenging to solve.

But with some creativity, we can use Monte Carlo for everything.

---

## What if we make lots of assumptions?

Assume that `$$\begin{aligned}p(x_0) &amp;= N(m_0, P_0) \\ p_k(x_k | x_{k-1}) &amp;= N(A_{k-1}x_{k-1}, Q_{k-1})\\ p_k(y_k | x_k) &amp;= N(H_k x_k, R_k)\end{aligned}.$$`

Then __all the ugly integrals have closed form representations__ by properties of conditional Gaussian distributions.

--

.pull-left[
Distributions:

$$
`\begin{aligned}
p(x_k | y_1,\ldots,y_{k-1}) &amp;= N(m^{-}_k, P^{-}_k)\\
p(x_k | y_1,\ldots,y_{k}) &amp;= N(m_k, P_k)\\
p(y_{k} | y_1,\ldots,y_{k-1}) &amp;= N(H_k m^-_k, S_k)\\
\end{aligned}`
$$
Prediction:
$$
`\begin{aligned}
m^-_k &amp;= A_{k-1}m_{k-1}\\
P^-_k &amp;= A_{k-1}P_{k-1}A^\mathsf{T}_{k-1} + Q_{k-1}
\end{aligned}`
$$

]

.pull-right[
Update:
$$
`\begin{aligned}
v_k &amp;= y_k - H_k m_k^-\\
S_k &amp;= H_k P_k^- H_k^\mathsf{T} + R_k\\
K_k &amp;= P^-_k H_k^\mathsf{T} S_k^{-1}\\
m_k &amp;= m^-_k + K_{k}v_{k}\\
P_k &amp;= P^-_k - K_k S_k K_k^\mathsf{T}
\end{aligned}`
$$

]

---

## Code or it isn't real (Kalman Filter)

.pull-left[

```r
kalman &lt;- function(y, m0, P0, A, Q, H, R) {
  n &lt;- length(y)
  m &lt;- double(n+1)
  P &lt;- double(n+1)
  m[1] &lt;- m0
  P[1] &lt;- P0
  for (k in seq(n)) {
    mm &lt;- A * m[k]
    Pm &lt;- A * P[k] * A + Q
    v &lt;- y[k] - H * mm
    S &lt;- H * Pm * H + R
    K &lt;- Pm * H / S
    m[k+1] &lt;- mm + K * v
    P[k+1] &lt;- Pm - K * S * K
  }
  tibble(t = 1:n, m = m[-1], P = P[-1])
}
```
]

.pull-right[

```r
set.seed(2022-06-01)
x &lt;- double(100)
for (k in 2:100) x[k] = x[k-1] + rnorm(1)
y &lt;- x + rnorm(100, sd = 1)
kf &lt;- kalman(y, 0, 5, 1, 1, 1, 1)
```

&lt;img src="gfx/plot-it-1.svg" width="100%" style="display: block; margin: auto;" /&gt;



]


---

## Important notes

* So far, we assumed all parameters were known

* "Parameters" here is a bit ambiguous: what is `\(x_k\)`?

* In reality, we had 6: `m0, P0, A, Q, H, R`

* I sort of also think of `\(x\)` as "parameters" in the Bayesian sense

* By that I mean, "latent variables for which we have prior distributions"

* What if we want to estimate them?

__Bayesian way__: `m0` and `P0` are already the parameters of for the prior on 
`\(x_1\)`. Put priors on the other 4.

__Frequentist way__: Just maximize the likelihood. Can technically take 
`P0` `\(\rightarrow\infty\)` to remove it and `m0`

--

The Likelihood is produced as a by-product of the Kalman Filter. 

`$$-\ell(\theta) = \sum_{k=1}^T \left(v_k^\mathsf{T}S_k^{-1}v_k + \log |S_k| + m \log 2\pi\right)$$`


---

## Smoothing (not in the chapter, whoops...)

* We also want `\(p(x_k | y_1,\ldots,y_{T})\)`

* Filtering went "forward" in time. At the end we got, 
`\(p(x_T | y_1,\ldots,y_{T})\)`. Smoothing starts there and goes "backward"

* For "everything linear Gaussian", this is again "easy"

* Set `\(m_T^s = m_T\)`, `\(P_T^s = P_T\)`. 

* For `\(k = T-1,\ldots,1\)`, 


`$$\begin{aligned}
G_k &amp;= P_k A_k^\mathsf{T} [P_{k+1}^-]^{-1}\\
m_k^s &amp;= m_k + G_k(m_{k+1}^s - m_{k+1}^-)\\
P_k^s &amp;= P_k + G_k(P_{k+1}^s - P_{k+1}^-)G_k^\mathsf{T}\\
x_k | y_1,\ldots,y_T &amp;= N(m^s_k, P_k^s)
\end{aligned}$$`

---

## Comparing the filter and the smoother

* Same data, different code (using a package)


```r
library(FKF)
filt &lt;- fkf(a0 = 0, P0 = matrix(5), dt = matrix(0), ct = matrix(0), 
            Tt = matrix(1), Zt = matrix(1), HHt = matrix(1), GGt = matrix(1), 
            yt = matrix(y, ncol = length(y)))
smo &lt;- fks(filt)
```

&lt;img src="gfx/plot-smooth-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

---

## What about non-linear and/or non-Gaussian

`$$\begin{aligned} x_k &amp;\sim p(x_k | x_{k-1}) \\ y_k &amp;\sim p(y_k | x_k)\end{aligned}$$`

Then we need to solve integrals. This is a pain. We approximate them. 

These all give **approximations to the filtering distribution**

* Extended Kalman filter - basically do a Taylor approximation, then do Kalman like
* Uncented Kalman filter - Approximate integrals with Sigma points
* Particle filter - Sequential Monte Carlo
* Bootstrap filter (simple version of SMC)
* Laplace Gaussian filter - Do a Laplace approximation to the distributions

---

## The bootstrap filter

* Need to **simulate** from the transition distribution (`rtrans`)
* Need to **evaluate** the observation distribution (`dobs`)


```r
boot_filter &lt;- 
  function(y, B = 1000, rtrans, dobs, a0 = 0, P0 = 1, perturb = function(x) x) 
    {
    n &lt;- length(y)
    filter_est &lt;- matrix(0, n, B)
    predict_est &lt;- matrix(0, n, B)
    init &lt;- rnorm(B, a0, P0)
    filter_est[1, ] = init
    for (i in seq(n)) {
      raw_w &lt;- dobs(y[i], filter_est[i, ])
      w &lt;- raw_w / sum(raw_w)
      selection &lt;- sample.int(B, replace = TRUE, prob = w)
      filter_est[i, ] &lt;- perturb(filter_est[i, selection])
      predict_est[i, ] &lt;- rtrans(filter_est[i, ])
      if (i &lt; n) filter_est[i+1, ] &lt;- predict_est[i, ]
    }
    list(filt = filter_est, pred = predict_est)
  }
```

---

## About the bootstrap filter

* Super simple. Works by using Monte Carlo approximations for all the integrals
* "Plug and play". All you do is specify 2 functions.
* Likelihood is a direct consequence.
* So if the functions depend on parameters, that's cool.

In the SIR:

* `rtrans =  ode_solve(x[t-1], theta)` - numerically propagates 1-step forward
* `dobs = dnbinom(y[t], x[t], theta)` - evaluate the 1-step likelihood

How do we estimate the parameters `\(\theta\)`?

* 0-th order optimization. Want to minimize `\(f(\theta)\)`, just evaluate `\(f\)` for `\(\theta \in \{\theta_1,\ldots,\theta_{huge}\}\)`, keep the best. `dobs` is the likelihood.
* 1-st order. Need `\(\nabla_\theta f(\theta)\)`.
* Bayesian. Like 0-th order, but we explore `\(\Theta\)` in a careful manner.

---

## Final notes

Suppose
`$$\begin{aligned}
x_k &amp;= Bx_{k-1} + \epsilon_k\\
y_k &amp;= Ax_{k} + \eta_k
\end{aligned}$$`

We can estimate `\(x_1,\ldots,x_T\)` with the Bootstrap filter.

--

`$$\hat{x} = \arg\min_x \frac{1}{2}\sum_{k=1}^T (y_k-Ax_k)^2 + \frac{\lambda}{2}
\sum_{k=2}^T(x_k - Bx_{k-1})^2$$`

Here, `\(\lambda = \sigma^2_\eta / \sigma^2_\epsilon\)`.

This is just "ridge regression". Exactly the same as "just compute `\(p(x_1,\ldots,x_T | y_1,\ldots,y_T)\)`".

&gt; if we don't care about the whole distribution, just write it as a penalized likelihood problem
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="src/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9",
"countIncrementalSlides": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
